__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
import os
import streamlit as st
from dotenv import load_dotenv
from langchain.vectorstores import Chroma
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA

@st.cache_resource
def load_chain():
    """
    è¼‰å…¥æ‰€æœ‰å¿…è¦çš„æ¨¡å‹å’Œ RAG éˆã€‚
    """
    print("--- æ­£åœ¨è¼‰å…¥æ¨¡å‹èˆ‡ RAG éˆ... ---")
    
    load_dotenv()
    persist_directory = 'chroma_db'
    if not os.path.isdir(persist_directory):
        raise FileNotFoundError(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°å‘é‡è³‡æ–™åº«ï¼\nè«‹ç¢ºèª 'chroma_db' è³‡æ–™å¤¾å­˜åœ¨æ–¼æ‚¨åŸ·è¡ŒæŒ‡ä»¤çš„ç›®éŒ„ï¼š{os.getcwd()}")
    if os.getenv("GOOGLE_API_KEY") is None:
        raise ValueError("éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° GOOGLE_API_KEYã€‚\nè«‹æª¢æŸ¥ä½ çš„ .env æª”æ¡ˆæ˜¯å¦å·²æ­£ç¢ºè¨­å®šã€‚")
        
    model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs={'device': 'cpu'}
    )
    
    vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
    
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=0)
    
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    
    # --- âœ¨âœ¨âœ¨ ä¿®æ”¹è™• 1 âœ¨âœ¨âœ¨ ---
    # æˆ‘å€‘åœ¨é€™è£¡åŠ å…¥ return_source_documents=True
    # è®“ RAG éˆå›å‚³å®ƒåƒè€ƒçš„åŸå§‹æ–‡ä»¶
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True  # <-- æ–°å¢é€™ä¸€è¡Œ
    )
    # -----------------------------
    print("--- æ¨¡å‹èˆ‡ RAG éˆè¼‰å…¥å®Œç•¢ ---")
    return qa_chain

def main():
    """
    Streamlit Web App çš„ä¸»é«”ã€‚
    """
    st.set_page_config(page_title="AI è²¡å ±åˆ†æå¸«", page_icon="ğŸ“ˆ")
    st.title("ğŸ“ˆ AI è²¡å ±åˆ†æå¸«")

    try:
        qa_chain = load_chain()
    except Exception as e:
        st.error(f"æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•å¤±æ•—ï¼š\n{e}")
        st.stop() 

    st.markdown("ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„ AI è²¡å ±åˆ†æåŠ©ç†ã€‚æˆ‘å·²ç¶“è®€å®Œäº†å°ç©é›» (TSMC) 2023 å¹´çš„ 20-F å¹´å ±ï¼Œä½ å¯ä»¥å•æˆ‘ä»»ä½•ç›¸é—œå•é¡Œã€‚")

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            # --- âœ¨âœ¨âœ¨ ä¿®æ”¹è™• 3 âœ¨âœ¨âœ¨ ---
            # å¦‚æœè¨Šæ¯æ˜¯ AI çš„ï¼Œä¸”åŒ…å«ä¾†æºï¼Œå°±é¡¯ç¤ºä¾†æº
            if message["role"] == "assistant" and "sources" in message:
                with st.expander("æŸ¥çœ‹å¼•ç”¨ä¾†æº"):
                    for source in message["sources"]:
                        st.markdown(f"**ä¾†æºé æ•¸ï¼š{source['page']}**")
                        st.markdown(f"> {source['content']}")
            # -----------------------------

    if prompt := st.chat_input("ä½ æƒ³å•ä»€éº¼ï¼Ÿ (ä¾‹å¦‚ï¼šå°ç©é›»çš„ä¸»è¦é¢¨éšªæ˜¯ä»€éº¼ï¼Ÿ)"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        with st.spinner("æ€è€ƒä¸­..."):
            try:
                # --- âœ¨âœ¨âœ¨ ä¿®æ”¹è™• 2 âœ¨âœ¨âœ¨ ---
                # .invoke ç¾åœ¨æœƒå›å‚³ä¸€å€‹åŒ…å« 'result' å’Œ 'source_documents' çš„å­—å…¸
                result = qa_chain.invoke(prompt)
                response_text = result["result"]
                
                # å–å¾—ä¾†æºæ–‡ä»¶ä¸¦è™•ç†
                sources = []
                if "source_documents" in result:
                    for doc in result["source_documents"]:
                        # PyPDFLoader çš„é ç¢¼æ˜¯å¾ 0 é–‹å§‹ï¼Œæˆ‘å€‘ +1 è®“å®ƒæ›´æ˜“è®€
                        page_num = doc.metadata.get('page', 'N/A')
                        if page_num != 'N/A':
                            page_num += 1
                        
                        sources.append({
                            "content": doc.page_content,
                            "page": page_num
                        })
                
                # é¡¯ç¤º AI å›æ‡‰
                with st.chat_message("assistant"):
                    st.markdown(response_text)
                    # åœ¨ AI å›æ‡‰ä¸‹æ–¹ï¼Œç«‹å³é¡¯ç¤ºä¾†æº
                    if sources:
                        with st.expander("æŸ¥çœ‹å¼•ç”¨ä¾†æº"):
                            for source in sources:
                                st.markdown(f"**ä¾†æºé æ•¸ï¼š{source['page']}**")
                                st.markdown(f"> {source['content']}")
                
                # å°‡ AI å›æ‡‰èˆ‡ä¾†æºä¸€èµ·å­˜å…¥èŠå¤©ç´€éŒ„
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": response_text,
                    "sources": sources  # <-- å°‡ä¾†æºä¹Ÿå­˜èµ·ä¾†
                })
                # -----------------------------
                
            except Exception as e:
                error_message = f"ç™¼ç”ŸéŒ¯èª¤ï¼š{e} \n\n(é€™å¾ˆå¯èƒ½æ˜¯å› ç‚º Google API çš„å…è²»é¡åº¦é”åˆ°äº†é€Ÿç‡é™åˆ¶ï¼Œè«‹ç¨å€™ä¸€åˆ†é˜å†è©¦ã€‚)"
                with st.chat_message("assistant"):
                    st.error(error_message)
                st.session_state.messages.append({"role": "assistant", "content": error_message})

if __name__ == "__main__":

    main()
